{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains a demo for Qdrant vector database\n",
    "\n",
    "Reference:\n",
    "https://api.qdrant.tech/api-reference \n",
    "https://github.com/qdrant/qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tm0792.STUDENTS.007\\AppData\\Roaming\\Python\\Python311\\site-packages\\vertexai\\_model_garden\\_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n",
      "C:\\Users\\tm0792.STUDENTS.007\\AppData\\Roaming\\Python\\Python311\\site-packages\\vertexai\\vision_models\\_vision_models.py:153: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding for frame_0.jpg (length 128):\n",
      "[0.0317403339, 0.0485488, -0.0125789735, -0.0115504358, -0.441019028] ...\n",
      "\n",
      "Embedding for frame_1.jpg (length 128):\n",
      "[0.0753487349, 0.00619010394, -0.051838439, 0.017776316, -0.456335813] ...\n",
      "\n",
      "Embedding for frame_10.jpg (length 128):\n",
      "[0.0765796155, 0.02871041, -0.0599593185, -0.0164518617, -0.455635] ...\n",
      "\n",
      "Embedding for frame_11.jpg (length 128):\n",
      "[0.040443249, 0.0347772874, -0.0184420217, 0.0105499672, -0.421787679] ...\n",
      "\n",
      "Embedding for frame_12.jpg (length 128):\n",
      "[0.0518494695, 0.00713314908, 0.00091019395, -0.0317014456, -0.443772644] ...\n",
      "\n",
      "Embedding for frame_13.jpg (length 128):\n",
      "[0.0849476904, 0.0239525754, -0.0130669596, -0.00410141144, -0.427604318] ...\n",
      "\n",
      "Embedding for frame_14.jpg (length 128):\n",
      "[0.0078506358, 0.0747402, -0.0248138458, -0.0493678898, -0.407604665] ...\n",
      "\n",
      "Embedding for frame_15.jpg (length 128):\n",
      "[0.126423389, 0.0398501307, -0.0688067526, -0.0121918013, -0.459890157] ...\n",
      "\n",
      "Embedding for frame_16.jpg (length 128):\n",
      "[0.0427845307, 0.057614591, 0.0147414329, -0.0840350911, -0.443623602] ...\n",
      "\n",
      "Embedding for frame_17.jpg (length 128):\n",
      "[0.0349638425, 0.093247, -0.0457390472, -0.014393853, -0.429223806] ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import vertexai\n",
    "from vertexai.vision_models import Image, MultiModalEmbeddingModel\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "# --- STEP 1: Init Vertex AI ---\n",
    "PROJECT_ID = \"verse-dev-433901\"\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "embedding_dimension = 128\n",
    "model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")\n",
    "\n",
    "# --- STEP 2: Init Qdrant Cloud Client ---\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://e079d073-2510-4214-b99c-507cc442e61f.us-east4-0.gcp.cloud.qdrant.io\",  # e.g., https://abc123.us-east-1-0.aws.cloud.qdrant.io\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.0DDbBi5cKy7Ldo-TIarLckW1_K3-x1P5tO3Oj9hcvNQ\",\n",
    ")\n",
    "\n",
    "# --- STEP 3: Create a collection (if not exists) ---\n",
    "collection_name = \"video_image_embeddings\"\n",
    "\n",
    "if not qdrant_client.collection_exists(collection_name=collection_name):\n",
    "    qdrant_client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=embedding_dimension, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "# --- STEP 4: Load top 10 images and generate embeddings ---\n",
    "frames_folder = \"frames\"\n",
    "image_files = sorted([f for f in os.listdir(frames_folder) if f.endswith(('.jpg', '.png'))])[:10]\n",
    "\n",
    "points = []\n",
    "\n",
    "for i, filename in enumerate(image_files):\n",
    "    image_path = os.path.join(frames_folder, filename)\n",
    "    image = Image.load_from_file(image_path)\n",
    "\n",
    "    embeddings = model.get_embeddings(\n",
    "        image=image,\n",
    "        dimension=embedding_dimension\n",
    "    )\n",
    "\n",
    "    vector = embeddings.image_embedding\n",
    "\n",
    "    point = PointStruct(\n",
    "        id=i,\n",
    "        vector=vector,\n",
    "        payload={\"filename\": filename, \"description\": f\"Frame from {filename}\"}\n",
    "    )\n",
    "\n",
    "    points.append(point)\n",
    "\n",
    "    print(f\"\\nEmbedding for {filename} (length {len(vector)}):\\n{vector[:5]} ...\")\n",
    "\n",
    "# --- STEP 5: Upload all points to Qdrant ---\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frame_0.jpg',\n",
       " 'frame_1.jpg',\n",
       " 'frame_10.jpg',\n",
       " 'frame_11.jpg',\n",
       " 'frame_12.jpg',\n",
       " 'frame_13.jpg',\n",
       " 'frame_14.jpg',\n",
       " 'frame_15.jpg',\n",
       " 'frame_16.jpg',\n",
       " 'frame_17.jpg']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0, Payload: {'filename': 'frame_0.jpg', 'description': 'Frame from frame_0.jpg'}\n",
      "ID: 1, Payload: {'filename': 'frame_1.jpg', 'description': 'Frame from frame_1.jpg'}\n",
      "ID: 2, Payload: {'filename': 'frame_10.jpg', 'description': 'Frame from frame_10.jpg'}\n",
      "ID: 3, Payload: {'filename': 'frame_11.jpg', 'description': 'Frame from frame_11.jpg'}\n",
      "ID: 4, Payload: {'filename': 'frame_12.jpg', 'description': 'Frame from frame_12.jpg'}\n",
      "ID: 5, Payload: {'filename': 'frame_13.jpg', 'description': 'Frame from frame_13.jpg'}\n",
      "ID: 6, Payload: {'filename': 'frame_14.jpg', 'description': 'Frame from frame_14.jpg'}\n",
      "ID: 7, Payload: {'filename': 'frame_15.jpg', 'description': 'Frame from frame_15.jpg'}\n",
      "ID: 8, Payload: {'filename': 'frame_16.jpg', 'description': 'Frame from frame_16.jpg'}\n",
      "ID: 9, Payload: {'filename': 'frame_17.jpg', 'description': 'Frame from frame_17.jpg'}\n"
     ]
    }
   ],
   "source": [
    "# Fetch all points in the collection\n",
    "\n",
    "from qdrant_client.http.models import ScrollRequest\n",
    "\n",
    "scroll_result = qdrant_client.scroll(\n",
    "    collection_name=collection_name,\n",
    "    scroll_filter=None,  # no filter means get everything\n",
    "    limit=10,            # number of points to retrieve at a time\n",
    ")\n",
    "\n",
    "for point in scroll_result[0]:  # scroll_result is (points, next_page_offset)\n",
    "    print(f\"ID: {point.id}, Payload: {point.payload}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tm0792.STUDENTS.007\\AppData\\Local\\Temp\\ipykernel_28364\\1611641662.py:14: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Match ID: 2, Score: 0.99999994, Payload: {'filename': 'frame_10.jpg', 'description': 'Frame from frame_10.jpg'}\n",
      "\n",
      "Match ID: 4, Score: 0.8903677, Payload: {'filename': 'frame_12.jpg', 'description': 'Frame from frame_12.jpg'}\n",
      "\n",
      "Match ID: 5, Score: 0.88858944, Payload: {'filename': 'frame_13.jpg', 'description': 'Frame from frame_13.jpg'}\n",
      "\n",
      "Match ID: 0, Score: 0.87942326, Payload: {'filename': 'frame_0.jpg', 'description': 'Frame from frame_0.jpg'}\n",
      "\n",
      "Match ID: 7, Score: 0.87876725, Payload: {'filename': 'frame_15.jpg', 'description': 'Frame from frame_15.jpg'}\n"
     ]
    }
   ],
   "source": [
    "# Search by image (similarity search)\n",
    "\n",
    "# Load a new query image\n",
    "query_image_path = r\"C:\\Users\\tm0792.STUDENTS.007\\OneDrive - UNT System\\Academics\\Vosyn\\Vosyn\\Cultural_Notes\\frames\\frame_10.jpg\"  # or reuse an existing image\n",
    "query_image = Image.load_from_file(query_image_path)\n",
    "\n",
    "# Get embedding\n",
    "query_embedding = model.get_embeddings(\n",
    "    image=query_image,\n",
    "    dimension=embedding_dimension\n",
    ").image_embedding\n",
    "\n",
    "# Perform vector search\n",
    "search_results = qdrant_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_embedding,\n",
    "    limit=5  # return top 5 matches\n",
    ")\n",
    "\n",
    "# Display search results\n",
    "for result in search_results:\n",
    "    print(f\"\\nMatch ID: {result.id}, Score: {result.score}, Payload: {result.payload}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Search using Metadata\n",
    "\n",
    "\n",
    "from qdrant_client.http.models import PayloadSchemaType\n",
    "\n",
    "qdrant_client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"filename\",\n",
    "    field_schema=PayloadSchemaType.KEYWORD\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue\n",
    "\n",
    "search_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"filename\",\n",
    "            match=MatchValue(value=\"frame_2.jpg\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = qdrant_client.scroll(\n",
    "    collection_name=collection_name,\n",
    "    scroll_filter=search_filter,\n",
    "    limit=1\n",
    ")\n",
    "\n",
    "print(result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Record(id=3, payload={'filename': 'frame_11.jpg', 'description': 'Frame from frame_11.jpg'}, vector=None, shard_key=None, order_value=None)]\n"
     ]
    }
   ],
   "source": [
    "# Get a specific point by ID\n",
    "\n",
    "point = qdrant_client.retrieve(\n",
    "    collection_name=collection_name,\n",
    "    ids=[3]\n",
    ")\n",
    "\n",
    "print(point)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
